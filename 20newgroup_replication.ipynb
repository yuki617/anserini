{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pyserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search import pysearch\n",
    "from pyserini.index import pyutils\n",
    "from pyserini.analysis.pyanalysis import get_lucene_analyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_utils_train = pyutils.IndexReaderUtils('lucene-index.20newsgroup.pos+docvectors+rawdocs')\n",
    "index_utils_test = pyutils.IndexReaderUtils('lucene-index.20newstest.pos+docvectors+rawdocs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 20Newsgroup Collection and Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding 20newsgroup collection in pyclass.py Jcollection, we can just call the two line code below to get collection\n",
    "from pyserini.collection import pycollection      \n",
    "collection = pycollection.Collection('TwentyNewsgroupsCollection', 'collection/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jnius import autoclass,cast\n",
    "from enum import Enum\n",
    "class JCollections(Enum):\n",
    "    TwentyNewsgroupsCollection = autoclass('io.anserini.collection.TwentyNewsgroupsCollection')\n",
    "TwentyNewsgroupsCollection = autoclass('io.anserini.collection.TwentyNewsgroupsCollection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "from pyserini.collection import pycollection\n",
    "JPaths = autoclass('java.nio.file.Paths')\n",
    "path = JPaths.get('../20news-bydate-train')\n",
    "class Collection:\n",
    "    def __init__(self, path):\n",
    "        self.collection_path = path\n",
    "        self.object = JCollections(TwentyNewsgroupsCollection).value(path)\n",
    "        self.collection_iterator = self.object.iterator()\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.collection_iterator.hasNext():\n",
    "            fs = self.collection_iterator.next()\n",
    "            return FileSegment(self, fs, fs.getSegmentPath())\n",
    "        else:\n",
    "            raise StopIteration\n",
    "class FileSegment:\n",
    "    \"\"\"\n",
    "    Iterable wrapper class for Anserini's FileSegment.\n",
    "    Parameters\n",
    "    ----------\n",
    "    collection : Collection\n",
    "        Parent collection of the file segment\n",
    "    segment : io.anserini.collection.FileSegment\n",
    "        FileSegment object to create wrapper from\n",
    "    segment_path : str\n",
    "        Path to file backing the file segment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, collection, segment, segment_path):\n",
    "        self.collection = collection\n",
    "        try:\n",
    "            self.object = cast(collection.object.getClass().getName() +\n",
    "                               '$Segment', segment)\n",
    "        except:\n",
    "            logger.exception('Exception from casting FileSegment type...')\n",
    "            self.object = cast('io.anserini.collection.FileSegment', segment)\n",
    "\n",
    "        self.segment_iterator = self.object.iterator()\n",
    "        self.segment_path = segment_path\n",
    "        self.segment_name = re.sub(r'\\\\|\\/', '-', collection.collection_path.relativize(segment_path).toString())\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.object.iterator().hasNext():\n",
    "            d = self.object.iterator().next()\n",
    "            return SourceDocument(self, d)\n",
    "        else:\n",
    "            # log if iteration stopped by error\n",
    "            if self.object.getErrorStatus():\n",
    "                logger.error(self.segment_name + ': Error from segment iteration, stopping...')\n",
    "                self.collection.counters.errors.increment()\n",
    "\n",
    "            # stop iteration and log skipped documents\n",
    "            skipped = self.object.getSkippedCount()\n",
    "            if skipped > 0:\n",
    "                self.collection.counters.skips.increment(skipped)\n",
    "                logger.warning(self.segment_name + ': ' + str(skipped) + ' documents skipped')\n",
    "            self.object.close()\n",
    "            raise StopIteration\n",
    "\n",
    "\n",
    "class SourceDocument:\n",
    "    \"\"\"\n",
    "    Wrapper class for Anserini's SourceDocument.\n",
    "    Parameters\n",
    "    ----------\n",
    "    segment : FileSegment\n",
    "        Parent segment of the source document\n",
    "    document : io.anserini.collection.SourceDocument\n",
    "        SourceDocument object to create wrapper from\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, segment, document):\n",
    "        self.segment = segment\n",
    "        self.object = document\n",
    "        self.id = self.object.id()\n",
    "        self.indexable = self.object.indexable()\n",
    "        self.contents = self.object.contents()\n",
    "        self.raw = self.object.raw()\n",
    "collection = Collection(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.index import pygenerator\n",
    "generator = pygenerator.Generator('DefaultLuceneDocumentGenerator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a matching relationship between the category number and docids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "category_list = []\n",
    "for root, dirs, files in os.walk(\"../20news-bydate-train\", topdown=False):\n",
    "    for name in dirs:\n",
    "        category_list.append(name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_lst = ['alt.atheism',\n",
    "'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    "'talk.religion.misc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "category_dic = {}\n",
    "for name in category_list:\n",
    "    category_dic[category_lst.index(name)] = []\n",
    "    for root, dirs, files in os.walk(\"../20news-bydate-train/\"+name, topdown=False):\n",
    "        for file in files:\n",
    "            category_dic[category_lst.index(name)].append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tfidf dataframe/target list for the training collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(docid,index_utils):\n",
    "    '''get a tiidf dictionary where key is the term and item is tfidf value'''\n",
    "    tfidf = {}\n",
    "    tf = index_utils.get_document_vector(docid)\n",
    "    analyzer = get_lucene_analyzer(stemming=False, stopwords=False)\n",
    "    df = {term: (index_utils.get_term_counts(term, analyzer=analyzer))[0] for term in tf.keys()}\n",
    "    idfDict = {}\n",
    "    for word, val in df.items():\n",
    "        # no idea why the value val can be 0, but this only happen if the word ends with . or '\n",
    "        if float(val) !=0:\n",
    "            idfDict[word] = math.log10(N / float(val) + 1)\n",
    "        else:\n",
    "            idfDict[word] = math.log10(N / 1 + 1)\n",
    "    for word, val in tf.items():\n",
    "        tfidf[word] = val*idfDict[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "tfidflist = []\n",
    "targetlist = []\n",
    "N = 11314 \n",
    "for (i, fs) in enumerate(collection):\n",
    "    for (j, doc) in enumerate(fs):\n",
    "        parsed = generator.create_document(doc)\n",
    "        docid = parsed.get('id') \n",
    "        for category, docname in category_dic.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "            if docid in docname:\n",
    "                '''every time we found the category of the document, \n",
    "                we append the category name to targetlist \n",
    "                , and add the corresponding tfidf dictionary to a tfidflist'''\n",
    "                targetlist.append(category)\n",
    "                tfidf_dic = get_tfidf(docid,index_utils_train)\n",
    "                tfidflist.append(tfidf_dic)\n",
    "                break;\n",
    "#convert the tfidf list to a dataframe\n",
    "tfidflist_df = pd.DataFrame(tfidflist)\n",
    "tfidflist_df = tfidflist_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 116285)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidflist_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(tfidflist_df, targetlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get test Collection and Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = JPaths.get('../20news-bydate-test')\n",
    "collection_test = Collection(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#category_dic should contain the matching relationship of all categories and documents in the test folder\n",
    "category_dic = {}\n",
    "for name in category_list:\n",
    "    category_dic[category_lst.index(name)] = []\n",
    "    for root, dirs, files in os.walk(\"../20news-bydate-test/\"+name, topdown=False):\n",
    "        for file in files:\n",
    "            category_dic[category_lst.index(name)].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tfidflist_test = []\n",
    "targetlist_test = []\n",
    "N = 11314\n",
    "for (i, fs) in enumerate(collection_test):\n",
    "    for (j, doc) in enumerate(fs):\n",
    "        parsed = generator.create_document(doc)\n",
    "        docid = parsed.get('id') \n",
    "        for category, docname in category_dic.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "            if docid in docname:\n",
    "                targetlist_test.append(category)\n",
    "                tfidf_dic = get_tfidf(docid,index_utils_test)\n",
    "                tfidflist_test.append(tfidf_dic)\n",
    "                break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the test tfidf_dataframe which has the same columns as train tfidf_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_test = pd.DataFrame(tfidflist_test,columns=tfidflist_df.columns)\n",
    "tfidf_df_test = tfidf_df_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7532, 116285)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the test target list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7919543281996814"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = clf.predict(tfidf_df_test)\n",
    "np.mean(predicted == targetlist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are in total 20 different categories.\n",
    "The following numbers in the vertical axis mean the index of that category in the category list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "category_lst = ['alt.atheism',\n",
    "'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    "'talk.religion.misc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.82      0.83       450\n",
      "          1       0.65      0.80      0.72       389\n",
      "          2       0.56      0.02      0.04       394\n",
      "          3       0.53      0.83      0.64       392\n",
      "          4       0.66      0.86      0.75       385\n",
      "          5       0.77      0.84      0.81       395\n",
      "          6       0.96      0.25      0.40       325\n",
      "          7       0.76      0.96      0.85       396\n",
      "          8       0.97      0.69      0.81       154\n",
      "          9       0.95      0.96      0.96       641\n",
      "         10       0.95      0.97      0.96       574\n",
      "         11       0.83      0.94      0.88       396\n",
      "         12       0.79      0.23      0.35       133\n",
      "         13       0.86      0.85      0.86       396\n",
      "         14       0.85      0.92      0.89       394\n",
      "         15       0.79      0.95      0.87       398\n",
      "         16       0.75      0.89      0.82       318\n",
      "         17       0.95      0.96      0.95       441\n",
      "         18       0.67      0.71      0.69       310\n",
      "         19       0.75      0.56      0.64       251\n",
      "\n",
      "avg / total       0.80      0.79      0.77      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(targetlist_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
